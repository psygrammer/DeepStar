{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 주 문서 링크\n",
    "\n",
    "두 개의 문서로 되어 있음. 뒷문서는 앞문서의 예제 파일을 이어서 사용함\n",
    "\n",
    "- [Building a Smart PySC2 Agent](https://chatbotslife.com/building-a-smart-pysc2-agent-cdc269cb095d)\n",
    "- [Add Smart Attacking to Your PySC2 Agent](https://itnext.io/add-smart-attacking-to-your-pysc2-agent-17fd5caad578)\n",
    "\n",
    "- 전체 구성은 이 [링크](http://www.lib4dev.in/info/skjb/pysc2-tutorial/101699713)를 참고\n",
    "\n",
    "## 단계\n",
    "\n",
    "### Building a Smart PySC2 Agent\n",
    "\n",
    "1. Create the Agent\n",
    "2. Define the Actions\n",
    "3. Define the State\n",
    "4. Define the Rewards\n",
    "5. It’s Alive!\n",
    "\n",
    "### Add Smart Attacking to Your PySC2 Agent\n",
    "\n",
    "1. Set Up\n",
    "2. Alter the Attack Action\n",
    "3. Simplify the Actions\n",
    "4. Add Enemy Positions\n",
    "5. Simplify the State\n",
    "6. SCV Attack Cheat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pysc2==1.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/31/228811689566b1de4e77f1019dbf15cd0cf377774774b0f822ae31f89609/PySC2-1.2-py3-none-any.whl (112kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 219kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: mpyq in /Users/j/anaconda3/lib/python3.7/site-packages (from pysc2==1.2) (0.2.5)\n",
      "Requirement already satisfied: future in /Users/j/anaconda3/lib/python3.7/site-packages (from pysc2==1.2) (0.17.1)\n",
      "Requirement already satisfied: mock in /Users/j/anaconda3/lib/python3.7/site-packages (from pysc2==1.2) (2.0.0)\n",
      "Requirement already satisfied: pygame in /Users/j/anaconda3/lib/python3.7/site-packages (from pysc2==1.2) (1.9.6)\n",
      "Requirement already satisfied: numpy>=1.10 in /Users/j/anaconda3/lib/python3.7/site-packages (from pysc2==1.2) (1.15.4)\n",
      "Requirement already satisfied: websocket-client in /Users/j/anaconda3/lib/python3.7/site-packages (from pysc2==1.2) (0.56.0)\n",
      "Requirement already satisfied: enum34 in /Users/j/anaconda3/lib/python3.7/site-packages (from pysc2==1.2) (1.1.6)\n",
      "Collecting futures (from pysc2==1.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/26/b61e3a4eb50653e8a7339d84eeaa46d1e93b92951978873c220ae64d0733/futures-3.1.1.tar.gz\n",
      "Requirement already satisfied: six in /Users/j/anaconda3/lib/python3.7/site-packages (from pysc2==1.2) (1.10.0)\n",
      "Requirement already satisfied: s2clientprotocol>=3.19.0.58400.0 in /Users/j/anaconda3/lib/python3.7/site-packages (from pysc2==1.2) (4.8.6.73620.0)\n",
      "Requirement already satisfied: portpicker>=1.2.0 in /Users/j/anaconda3/lib/python3.7/site-packages (from pysc2==1.2) (1.3.1)\n",
      "Requirement already satisfied: protobuf>=2.6 in /Users/j/anaconda3/lib/python3.7/site-packages (from pysc2==1.2) (3.7.1)\n",
      "Requirement already satisfied: absl-py>=0.1.0 in /Users/j/anaconda3/lib/python3.7/site-packages (from pysc2==1.2) (0.7.1)\n",
      "Requirement already satisfied: pbr>=0.11 in /Users/j/anaconda3/lib/python3.7/site-packages (from mock->pysc2==1.2) (1.10.0)\n",
      "Requirement already satisfied: setuptools in /Users/j/anaconda3/lib/python3.7/site-packages (from protobuf>=2.6->pysc2==1.2) (40.6.3)\n",
      "Building wheels for collected packages: futures\n",
      "  Building wheel for futures (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/j/Library/Caches/pip/wheels/f3/f9/c7/4fbf1faa6038faf183f6e3ea61f17a5f7eea5ab9a1dd7753fd\n",
      "Successfully built futures\n",
      "\u001b[31mERROR: smac 0.1.0b1 has requirement pysc2>=2.0.2, but you'll have pysc2 1.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: futures, pysc2\n",
      "  Found existing installation: PySC2 2.0.2\n",
      "    Uninstalling PySC2-2.0.2:\n",
      "      Successfully uninstalled PySC2-2.0.2\n",
      "Successfully installed futures-3.1.1 pysc2-1.2\n"
     ]
    }
   ],
   "source": [
    "# 이 파트는 pysc2 v1.2 기반으로 작성됨. 따라서 이 부분은 pysc2 ver1.2 를 설치해야 함\n",
    "\n",
    "!pip install pysc2==1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Smart PySC2 Agent\n",
    "\n",
    "## Create the Agent\n",
    "\n",
    "- 주요 상수와 클래스 셋업. 테란용으로 기술되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 부분은 발제자 import 문제로 추가되어 있는 것임. 다른 분들은 실행할 필요 없음\n",
    "import sys\n",
    "sys.path.append(\"/Users/j/Documents/seminar/2019/DeepStar/venv/lib/python3.7/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.lib import actions\n",
    "from pysc2.lib import features\n",
    "\n",
    "_NO_OP = actions.FUNCTIONS.no_op.id\n",
    "_SELECT_POINT = actions.FUNCTIONS.select_point.id\n",
    "_BUILD_SUPPLY_DEPOT = actions.FUNCTIONS.Build_SupplyDepot_screen.id\n",
    "_BUILD_BARRACKS = actions.FUNCTIONS.Build_Barracks_screen.id\n",
    "_TRAIN_MARINE = actions.FUNCTIONS.Train_Marine_quick.id\n",
    "_SELECT_ARMY = actions.FUNCTIONS.select_army.id\n",
    "_ATTACK_MINIMAP = actions.FUNCTIONS.Attack_minimap.id\n",
    "\n",
    "_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index\n",
    "_UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index\n",
    "_PLAYER_ID = features.SCREEN_FEATURES.player_id.index\n",
    "\n",
    "_PLAYER_SELF = 1\n",
    "\n",
    "_TERRAN_COMMANDCENTER = 18\n",
    "_TERRAN_SCV = 45 \n",
    "_TERRAN_SUPPLY_DEPOT = 19\n",
    "_TERRAN_BARRACKS = 21\n",
    "\n",
    "_NOT_QUEUED = [0]\n",
    "_QUEUED = [1]\n",
    "\n",
    "class SmartAgent_step1a(base_agent.BaseAgent):\n",
    "    def transformLocation(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(SmartAgent, self).step(obs)\n",
    "        \n",
    "        player_y, player_x = (obs.observation['minimap'][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()\n",
    "        self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "        \n",
    "        return actions.FunctionCall(_NO_OP, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```Q-Learning table class```를 추가. 여기에서 상태와 행동을 추적 관리\n",
    "- 이 클래스가 agent의 두뇌에 해당한다고 할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stolen from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            state_action = self.q_table.ix[observation, :]\n",
    "            \n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            \n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "        \n",
    "        q_predict = self.q_table.ix[s, a]\n",
    "        q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "        \n",
    "        # update\n",
    "        self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "I0610 16:54:25.795827 4579366336 sc_process.py:148] Connection attempt 0 (running: None)\n",
      "I0610 16:54:27.002763 4579366336 sc_process.py:148] Connection attempt 1 (running: None)\n",
      "err = 3 /opt/blizzard/buildserver/data.noindex/repositories/sc2/branches/SC2.4.9.a/Game/Contrib/macteam/libs/ClampCursor/Contrib/mach_override/mach_override.c:244\n",
      "err = 3 /opt/blizzard/buildserver/data.noindex/repositories/sc2/branches/SC2.4.9.a/Game/Contrib/macteam/libs/ClampCursor/Contrib/mach_override/mach_override.c:258\n",
      "err = 3 /opt/blizzard/buildserver/data.noindex/repositories/sc2/branches/SC2.4.9.a/Game/Contrib/macteam/libs/ClampCursor/Contrib/mach_override/mach_override.c:263\n",
      "I0610 16:54:29.120488 4579366336 sc_process.py:148] Connection attempt 2 (running: None)\n",
      "I0610 16:54:31.228204 4579366336 sc_process.py:148] Connection attempt 3 (running: None)\n",
      "I0610 16:54:32.945276 4579366336 sc_process.py:148] Connection attempt 4 (running: None)\n",
      "I0610 16:54:35.082213 4579366336 sc_process.py:148] Connection attempt 5 (running: None)\n",
      "I0610 16:54:51.029213 4579366336 sc2_env.py:200] Environment is ready.\n",
      "I0610 16:54:51.035480 4579366336 sc2_env.py:240] Starting episode: 1\n",
      "   0/no_op                                              ()\n",
      "   1/move_camera                                        (1/minimap [64, 64])\n",
      "   2/select_point                                       (6/select_point_act [4]; 0/screen [84, 84])\n",
      "   3/select_rect                                        (7/select_add [2]; 0/screen [84, 84]; 2/screen2 [84, 84])\n",
      "   4/select_control_group                               (4/control_group_act [5]; 5/control_group_id [10])\n",
      " 453/Stop_quick                                         (3/queued [2])\n",
      " 230/Effect_Spray_screen                                (3/queued [2]; 0/screen [84, 84])\n",
      " 264/Harvest_Gather_screen                              (3/queued [2]; 0/screen [84, 84])\n",
      " 331/Move_screen                                        (3/queued [2]; 0/screen [84, 84])\n",
      " 332/Move_minimap                                       (3/queued [2]; 1/minimap [64, 64])\n",
      " 333/Patrol_screen                                      (3/queued [2]; 0/screen [84, 84])\n",
      " 334/Patrol_minimap                                     (3/queued [2]; 1/minimap [64, 64])\n",
      "  12/Attack_screen                                      (3/queued [2]; 0/screen [84, 84])\n",
      "  13/Attack_minimap                                     (3/queued [2]; 1/minimap [64, 64])\n",
      " 274/HoldPosition_quick                                 (3/queued [2])\n",
      " 220/Effect_Repair_screen                               (3/queued [2]; 0/screen [84, 84])\n",
      " 221/Effect_Repair_autocast                             ()\n",
      " 269/Harvest_Return_quick                               (3/queued [2])\n",
      "  79/Build_Refinery_screen                              (3/queued [2]; 0/screen [84, 84])\n",
      "  91/Build_SupplyDepot_screen                           (3/queued [2]; 0/screen [84, 84])\n",
      " 261/Halt_quick                                         (3/queued [2])\n",
      "  50/Build_EngineeringBay_screen                        (3/queued [2]; 0/screen [84, 84])\n",
      "  42/Build_Barracks_screen                              (3/queued [2]; 0/screen [84, 84])\n",
      " 140/Cancel_quick                                       (3/queued [2])\n",
      " 335/Rally_Units_screen                                 (3/queued [2]; 0/screen [84, 84])\n",
      " 336/Rally_Units_minimap                                (3/queued [2]; 1/minimap [64, 64])\n",
      " 281/Lift_quick                                         (3/queued [2])\n",
      " 477/Train_Marine_quick                                 (3/queued [2])\n",
      " 168/Cancel_Last_quick                                  (3/queued [2])\n",
      "  11/build_queue                                        (11/build_queue_id [10])\n",
      "   7/select_army                                        (7/select_add [2])\n",
      "   5/select_unit                                        (8/select_unit_act [4]; 9/select_unit_id [500])\n",
      "I0610 16:55:23.398778 4579366336 sc2_env.py:310] Episode finished. Outcome: [-1], reward: [-1], score: [0]\n",
      "I0610 16:55:27.407500 4579366336 sc2_env.py:240] Starting episode: 2\n",
      "I0610 16:55:29.107331 4579366336 sc2_env.py:310] Episode finished. Outcome: [-1], reward: [-1], score: [1235]\n",
      "Took 39.692 seconds for 1644 steps: 41.419 fps\n",
      "I0610 16:55:30.727401 4579366336 sc2_env.py:327] Environment Close\n",
      "I0610 16:55:30.727540 4579366336 sc2_env.py:342] \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/lib/protocol.py\", line 61, in catch_websocket_connection_errors\n",
      "    yield\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/lib/protocol.py\", line 152, in _read\n",
      "    response_str = self._sock.recv()\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/websocket/_core.py\", line 313, in recv\n",
      "    opcode, data = self.recv_data()\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/websocket/_core.py\", line 330, in recv_data\n",
      "    opcode, frame = self.recv_data_frame(control_frame)\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/websocket/_core.py\", line 343, in recv_data_frame\n",
      "    frame = self.recv_frame()\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/websocket/_core.py\", line 377, in recv_frame\n",
      "    return self.frame_buffer.recv_frame()\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/websocket/_abnf.py\", line 361, in recv_frame\n",
      "    self.recv_header()\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/websocket/_abnf.py\", line 309, in recv_header\n",
      "    header = self.recv_strict(2)\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/websocket/_abnf.py\", line 396, in recv_strict\n",
      "    bytes_ = self.recv(min(16384, shortage))\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/websocket/_core.py\", line 452, in _recv\n",
      "    return recv(self.sock, bufsize)\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/websocket/_socket.py\", line 115, in recv\n",
      "    \"Connection is already closed.\")\n",
      "websocket._exceptions.WebSocketConnectionClosedException: Connection is already closed.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/bin/agent.py\", line 112, in <module>\n",
      "    app.run(main)\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 300, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/bin/agent.py\", line 98, in main\n",
      "    run_thread(agent_cls, FLAGS.map, FLAGS.render)\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/bin/agent.py\", line 77, in run_thread\n",
      "    run_loop.run_loop([agent], env, FLAGS.max_agent_steps)\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/env/run_loop.py\", line 35, in run_loop\n",
      "    timesteps = env.reset()\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/env/base_env_wrapper.py\", line 39, in reset\n",
      "    return self._env.reset(*args, **kwargs)\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/lib/stopwatch.py\", line 197, in _stopwatch\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/env/sc2_env.py\", line 237, in reset\n",
      "    self._restart()\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/env/sc2_env.py\", line 229, in _restart\n",
      "    self._controllers[0].restart()\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/lib/remote_controller.py\", line 80, in _valid_status\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/lib/remote_controller.py\", line 55, in _check_error\n",
      "    return check_error(func(*args, **kwargs), error_enum)\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/lib/stopwatch.py\", line 197, in _stopwatch\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/lib/remote_controller.py\", line 127, in restart\n",
      "    return self._client.send(restart_game=sc_pb.RequestRestartGame())\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/lib/protocol.py\", line 127, in send\n",
      "    res = self.send_req(sc_pb.Request(**kwargs))\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/lib/protocol.py\", line 113, in send_req\n",
      "    return self.read()\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/lib/stopwatch.py\", line 197, in _stopwatch\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/lib/protocol.py\", line 86, in read\n",
      "    response = self._read()\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/lib/protocol.py\", line 152, in _read\n",
      "    response_str = self._sock.recv()\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/contextlib.py\", line 130, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/j/anaconda3/lib/python3.7/site-packages/pysc2/lib/protocol.py\", line 63, in catch_websocket_connection_errors\n",
      "    raise ConnectionError(\"Connection already closed. SC2 probably crashed. \"\n",
      "pysc2.lib.protocol.ConnectionError: Connection already closed. SC2 probably crashed. Check the error log.\n",
      "I0610 16:55:30.758768 4579366336 sc2_env.py:327] Environment Close\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0610 16:55:30.758919 4579366336 sc2_env.py:342] \n",
      "I0610 16:55:30.758985 4579366336 sc2_env.py:327] Environment Close\n",
      "I0610 16:55:30.759028 4579366336 sc2_env.py:342] \n",
      "W0610 16:55:33.950988 4579366336 sc_process.py:183] Killing the process.\n",
      "I0610 16:55:34.133602 4579366336 sc_process.py:166] Shutdown with return code: -9\n"
     ]
    }
   ],
   "source": [
    "!python -m pysc2.bin.agent \\\n",
    "--map Simple64 \\\n",
    "--agent smart_agent.SmartAgent \\\n",
    "--agent_race T \\\n",
    "--max_agent_steps 0 \\\n",
    "--norender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 행동 정의하기\n",
    "\n",
    "- do nothing 도 유용한 행동임. 이 행동이 필요할 때도 있기 때문임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_SELECT_SCV = 'selectscv'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_SELECT_BARRACKS = 'selectbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_SELECT_ARMY = 'selectarmy'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_SELECT_SCV,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_SELECT_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "    ACTION_SELECT_ARMY,\n",
    "    ACTION_ATTACK,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```step()```에 임의로 행동 결정하는 모듈을 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartAgent_step2b(base_agent.BaseAgent):\n",
    "    def transformLocation(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(SmartAgent, self).step(obs)\n",
    "        \n",
    "        player_y, player_x = (obs.observation['minimap'][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()\n",
    "        self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "        \n",
    "        smart_action = smart_actions[random.randrange(0, len(smart_actions) - 1)]\n",
    "        \n",
    "        if smart_action == ACTION_DO_NOTHING:\n",
    "            return actions.FunctionCall(_NO_OP, [])\n",
    "\n",
    "        elif smart_action == ACTION_SELECT_SCV:\n",
    "            unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "            unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()\n",
    "                \n",
    "            if unit_y.any():\n",
    "                i = random.randint(0, len(unit_y) - 1)\n",
    "                target = [unit_x[i], unit_y[i]]\n",
    "                \n",
    "                return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "            if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:\n",
    "                unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "                unit_y, unit_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()\n",
    "                \n",
    "                if unit_y.any():\n",
    "                    target = self.transformLocation(int(unit_x.mean()), 0, int(unit_y.mean()), 20)\n",
    "                \n",
    "                    return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "            if _BUILD_BARRACKS in obs.observation['available_actions']:\n",
    "                unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "                unit_y, unit_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()\n",
    "                \n",
    "                if unit_y.any():\n",
    "                    target = self.transformLocation(int(unit_x.mean()), 20, int(unit_y.mean()), 0)\n",
    "            \n",
    "                    return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])\n",
    "    \n",
    "        elif smart_action == ACTION_SELECT_BARRACKS:\n",
    "            unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "            unit_y, unit_x = (unit_type == _TERRAN_BARRACKS).nonzero()\n",
    "                \n",
    "            if unit_y.any():\n",
    "                target = [int(unit_x.mean()), int(unit_y.mean())]\n",
    "        \n",
    "                return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_MARINE:\n",
    "            if _TRAIN_MARINE in obs.observation['available_actions']:\n",
    "                return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])\n",
    "        \n",
    "        elif smart_action == ACTION_SELECT_ARMY:\n",
    "            if _SELECT_ARMY in obs.observation['available_actions']:\n",
    "                return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])\n",
    "        \n",
    "        elif smart_action == ACTION_ATTACK:\n",
    "            if _ATTACK_MINIMAP in obs.observation[\"available_actions\"]:\n",
    "                if self.base_top_left:\n",
    "                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, [39, 45]])\n",
    "            \n",
    "                return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, [21, 24]])\n",
    "            \n",
    "        return actions.FunctionCall(_NO_OP, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-Learning Tb 추가\n",
    "```python\n",
    "    def __init__(self):\n",
    "        super(SmartAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상태 정의\n",
    "\n",
    "- QL에서 사용할 상태 이름을 정의\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartAgent_step3a(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(SmartAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "    \n",
    "    def transformLocation(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(SmartAgent, self).step(obs)\n",
    "        \n",
    "        player_y, player_x = (obs.observation['minimap'][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()\n",
    "        self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "        \n",
    "        ##################\n",
    "        unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "\n",
    "        depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()\n",
    "        supply_depot_count = 1 if depot_y.any() else 0\n",
    "\n",
    "        barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()\n",
    "        barracks_count = 1 if barracks_y.any() else 0\n",
    "            \n",
    "        supply_limit = obs.observation['player'][4]\n",
    "        army_supply = obs.observation['player'][5]\n",
    "        \n",
    "        current_state = [\n",
    "            supply_depot_count,\n",
    "            barracks_count,\n",
    "            supply_limit,\n",
    "            army_supply,\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        #smart_action = smart_actions[random.randrange(0, len(smart_actions) - 1)]\n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "        \n",
    "        ####################\n",
    "        \n",
    "        if smart_action == ACTION_DO_NOTHING:\n",
    "            return actions.FunctionCall(_NO_OP, [])\n",
    "\n",
    "        elif smart_action == ACTION_SELECT_SCV:\n",
    "            unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "            unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()\n",
    "                \n",
    "            if unit_y.any():\n",
    "                i = random.randint(0, len(unit_y) - 1)\n",
    "                target = [unit_x[i], unit_y[i]]\n",
    "                \n",
    "                return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "            if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:\n",
    "                unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "                unit_y, unit_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()\n",
    "                \n",
    "                if unit_y.any():\n",
    "                    target = self.transformLocation(int(unit_x.mean()), 0, int(unit_y.mean()), 20)\n",
    "                \n",
    "                    return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "            if _BUILD_BARRACKS in obs.observation['available_actions']:\n",
    "                unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "                unit_y, unit_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()\n",
    "                \n",
    "                if unit_y.any():\n",
    "                    target = self.transformLocation(int(unit_x.mean()), 20, int(unit_y.mean()), 0)\n",
    "            \n",
    "                    return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])\n",
    "    \n",
    "        elif smart_action == ACTION_SELECT_BARRACKS:\n",
    "            unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "            unit_y, unit_x = (unit_type == _TERRAN_BARRACKS).nonzero()\n",
    "                \n",
    "            if unit_y.any():\n",
    "                target = [int(unit_x.mean()), int(unit_y.mean())]\n",
    "        \n",
    "                return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_MARINE:\n",
    "            if _TRAIN_MARINE in obs.observation['available_actions']:\n",
    "                return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])\n",
    "        \n",
    "        elif smart_action == ACTION_SELECT_ARMY:\n",
    "            if _SELECT_ARMY in obs.observation['available_actions']:\n",
    "                return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])\n",
    "        \n",
    "        elif smart_action == ACTION_ATTACK:\n",
    "            if _ATTACK_MINIMAP in obs.observation[\"available_actions\"]:\n",
    "                if self.base_top_left:\n",
    "                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, [39, 45]])\n",
    "            \n",
    "                return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, [21, 24]])\n",
    "            \n",
    "        return actions.FunctionCall(_NO_OP, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 보상 정의\n",
    "\n",
    "- global 상수부에 아래 줄 추가\n",
    "- QL에서 스코어를 담당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "KILL_UNIT_REWARD = 0.2\n",
    "KILL_BUILDING_REWARD = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 스코어 반영을 위해 ```step()``` 에 유지하기 위한 단조증가하는 스코어를 유지\n",
    "```python\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SmartAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_killed_unit_score = 0\n",
    "        self.previous_killed_building_score = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 스코어 유지부\n",
    "\n",
    "```python\n",
    "\n",
    "        killed_unit_score = obs.observation['score_cumulative'][5]\n",
    "        killed_building_score = obs.observation['score_cumulative'][6]\n",
    "        \n",
    "        current_state = [\n",
    "            supply_depot_count,\n",
    "            barracks_count,\n",
    "            supply_limit,\n",
    "            army_supply,\n",
    "        ]\n",
    "        \n",
    "        reward = 0\n",
    "            \n",
    "        if killed_unit_score > self.previous_killed_unit_score:\n",
    "            reward += KILL_UNIT_REWARD\n",
    "                \n",
    "        if killed_building_score > self.previous_killed_building_score:\n",
    "            reward += KILL_BUILDING_REWARD\n",
    "                \n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- update score\n",
    "\n",
    "```python\n",
    "\n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "        \n",
    "        self.previous_killed_unit_score = killed_unit_score\n",
    "        self.previous_killed_building_score = killed_building_score\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 처리\n",
    "\n",
    "- 선택한 액션의 결과를 학습할 수 있게 하기 위해 이전 상태와 행동을 정의\n",
    "\n",
    "```python\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SmartAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_killed_unit_score = 0\n",
    "        self.previous_killed_building_score = 0\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- QL에 지금까지 정의한 스코어 관련부들을 모두 추가\n",
    "\n",
    "```python\n",
    "\n",
    "        current_state = [\n",
    "            supply_depot_count,\n",
    "            barracks_count,\n",
    "            supply_limit,\n",
    "            army_supply,\n",
    "        ]\n",
    "        \n",
    "        if self.previous_action is not None:\n",
    "            reward = 0\n",
    "                \n",
    "            if killed_unit_score > self.previous_killed_unit_score:\n",
    "                reward += KILL_UNIT_REWARD\n",
    "                    \n",
    "            if killed_building_score > self.previous_killed_building_score:\n",
    "                reward += KILL_BUILDING_REWARD\n",
    "                \n",
    "            self.qlearn.learn(str(self.previous_state), self.previous_action, reward, str(current_state))\n",
    "        \n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 코드는 smart_agent.py임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.lib import actions\n",
    "from pysc2.lib import features\n",
    "\n",
    "_NO_OP = actions.FUNCTIONS.no_op.id\n",
    "_SELECT_POINT = actions.FUNCTIONS.select_point.id\n",
    "_BUILD_SUPPLY_DEPOT = actions.FUNCTIONS.Build_SupplyDepot_screen.id\n",
    "_BUILD_BARRACKS = actions.FUNCTIONS.Build_Barracks_screen.id\n",
    "_TRAIN_MARINE = actions.FUNCTIONS.Train_Marine_quick.id\n",
    "_SELECT_ARMY = actions.FUNCTIONS.select_army.id\n",
    "_ATTACK_MINIMAP = actions.FUNCTIONS.Attack_minimap.id\n",
    "\n",
    "_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index\n",
    "_UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index\n",
    "_PLAYER_ID = features.SCREEN_FEATURES.player_id.index\n",
    "\n",
    "_PLAYER_SELF = 1\n",
    "\n",
    "_TERRAN_COMMANDCENTER = 18\n",
    "_TERRAN_SCV = 45 \n",
    "_TERRAN_SUPPLY_DEPOT = 19\n",
    "_TERRAN_BARRACKS = 21\n",
    "\n",
    "_NOT_QUEUED = [0]\n",
    "_QUEUED = [1]\n",
    "\n",
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_SELECT_SCV = 'selectscv'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_SELECT_BARRACKS = 'selectbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_SELECT_ARMY = 'selectarmy'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_SELECT_SCV,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_SELECT_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "    ACTION_SELECT_ARMY,\n",
    "    ACTION_ATTACK,\n",
    "]\n",
    "\n",
    "KILL_UNIT_REWARD = 0.2\n",
    "KILL_BUILDING_REWARD = 0.5\n",
    "\n",
    "# Stolen from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            state_action = self.q_table.ix[observation, :]\n",
    "            \n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            \n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "        \n",
    "        q_predict = self.q_table.ix[s, a]\n",
    "        q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "        \n",
    "        # update\n",
    "        self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))\n",
    "\n",
    "class SmartAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(SmartAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_killed_unit_score = 0\n",
    "        self.previous_killed_building_score = 0\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "        \n",
    "    def transformLocation(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "        \n",
    "    def step(self, obs):\n",
    "        super(SmartAgent, self).step(obs)\n",
    "        \n",
    "        player_y, player_x = (obs.observation['minimap'][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()\n",
    "        self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "        \n",
    "        unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "\n",
    "        depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()\n",
    "        supply_depot_count = supply_depot_count = 1 if depot_y.any() else 0\n",
    "\n",
    "        barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()\n",
    "        barracks_count = 1 if barracks_y.any() else 0\n",
    "            \n",
    "        supply_limit = obs.observation['player'][4]\n",
    "        army_supply = obs.observation['player'][5]\n",
    "        \n",
    "        killed_unit_score = obs.observation['score_cumulative'][5]\n",
    "        killed_building_score = obs.observation['score_cumulative'][6]\n",
    "        \n",
    "        current_state = [\n",
    "            supply_depot_count,\n",
    "            barracks_count,\n",
    "            supply_limit,\n",
    "            army_supply,\n",
    "        ]\n",
    "        \n",
    "        if self.previous_action is not None:\n",
    "            reward = 0\n",
    "                \n",
    "            if killed_unit_score > self.previous_killed_unit_score:\n",
    "                reward += KILL_UNIT_REWARD\n",
    "                    \n",
    "            if killed_building_score > self.previous_killed_building_score:\n",
    "                reward += KILL_BUILDING_REWARD\n",
    "                \n",
    "            self.qlearn.learn(str(self.previous_state), self.previous_action, reward, str(current_state))\n",
    "        \n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "        \n",
    "        self.previous_killed_unit_score = killed_unit_score\n",
    "        self.previous_killed_building_score = killed_building_score\n",
    "        self.previous_state = current_state\n",
    "        self.previous_action = rl_action\n",
    "        \n",
    "        if smart_action == ACTION_DO_NOTHING:\n",
    "            return actions.FunctionCall(_NO_OP, [])\n",
    "\n",
    "        elif smart_action == ACTION_SELECT_SCV:\n",
    "            unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "            unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()\n",
    "                \n",
    "            if unit_y.any():\n",
    "                i = random.randint(0, len(unit_y) - 1)\n",
    "                target = [unit_x[i], unit_y[i]]\n",
    "                \n",
    "                return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "            if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:\n",
    "                unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "                unit_y, unit_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()\n",
    "                \n",
    "                if unit_y.any():\n",
    "                    target = self.transformLocation(int(unit_x.mean()), 0, int(unit_y.mean()), 20)\n",
    "                \n",
    "                    return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "            if _BUILD_BARRACKS in obs.observation['available_actions']:\n",
    "                unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "                unit_y, unit_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()\n",
    "                \n",
    "                if unit_y.any():\n",
    "                    target = self.transformLocation(int(unit_x.mean()), 20, int(unit_y.mean()), 0)\n",
    "            \n",
    "                    return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])\n",
    "    \n",
    "        elif smart_action == ACTION_SELECT_BARRACKS:\n",
    "            unit_type = obs.observation['screen'][_UNIT_TYPE]\n",
    "            unit_y, unit_x = (unit_type == _TERRAN_BARRACKS).nonzero()\n",
    "                \n",
    "            if unit_y.any():\n",
    "                target = [int(unit_x.mean()), int(unit_y.mean())]\n",
    "        \n",
    "                return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_MARINE:\n",
    "            if _TRAIN_MARINE in obs.observation['available_actions']:\n",
    "                return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])\n",
    "        \n",
    "        elif smart_action == ACTION_SELECT_ARMY:\n",
    "            if _SELECT_ARMY in obs.observation['available_actions']:\n",
    "                return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])\n",
    "        \n",
    "        elif smart_action == ACTION_ATTACK:\n",
    "            if _ATTACK_MINIMAP in obs.observation[\"available_actions\"]:\n",
    "                if self.base_top_left:\n",
    "                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, [39, 45]])\n",
    "            \n",
    "                return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, [21, 24]])\n",
    "        \n",
    "        return actions.FunctionCall(_NO_OP, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "I0610 16:55:46.486043 4560528832 sc_process.py:148] Connection attempt 0 (running: None)\n",
      "err = 3 /opt/blizzard/buildserver/data.noindex/repositories/sc2/branches/SC2.4.9.a/Game/Contrib/macteam/libs/ClampCursor/Contrib/mach_override/mach_override.c:244\n",
      "err = 3 /opt/blizzard/buildserver/data.noindex/repositories/sc2/branches/SC2.4.9.a/Game/Contrib/macteam/libs/ClampCursor/Contrib/mach_override/mach_override.c:258\n",
      "err = 3 /opt/blizzard/buildserver/data.noindex/repositories/sc2/branches/SC2.4.9.a/Game/Contrib/macteam/libs/ClampCursor/Contrib/mach_override/mach_override.c:263\n",
      "I0610 16:55:51.028629 4560528832 sc_process.py:148] Connection attempt 1 (running: None)\n",
      "I0610 16:55:53.141706 4560528832 sc_process.py:148] Connection attempt 2 (running: None)\n",
      "I0610 16:55:55.251165 4560528832 sc_process.py:148] Connection attempt 3 (running: None)\n",
      "I0610 16:55:56.956897 4560528832 sc_process.py:148] Connection attempt 4 (running: None)\n",
      "I0610 16:55:59.066981 4560528832 sc_process.py:148] Connection attempt 5 (running: None)\n",
      "I0610 16:56:09.103901 4560528832 sc2_env.py:200] Environment is ready.\n",
      "I0610 16:56:09.111202 4560528832 sc2_env.py:240] Starting episode: 1\n",
      "   0/no_op                                              ()\n",
      "   1/move_camera                                        (1/minimap [64, 64])\n",
      "   2/select_point                                       (6/select_point_act [4]; 0/screen [84, 84])\n",
      "   3/select_rect                                        (7/select_add [2]; 0/screen [84, 84]; 2/screen2 [84, 84])\n",
      "   4/select_control_group                               (4/control_group_act [5]; 5/control_group_id [10])\n",
      " 453/Stop_quick                                         (3/queued [2])\n",
      " 230/Effect_Spray_screen                                (3/queued [2]; 0/screen [84, 84])\n",
      " 264/Harvest_Gather_screen                              (3/queued [2]; 0/screen [84, 84])\n",
      " 331/Move_screen                                        (3/queued [2]; 0/screen [84, 84])\n",
      " 332/Move_minimap                                       (3/queued [2]; 1/minimap [64, 64])\n",
      " 333/Patrol_screen                                      (3/queued [2]; 0/screen [84, 84])\n",
      " 334/Patrol_minimap                                     (3/queued [2]; 1/minimap [64, 64])\n",
      "  12/Attack_screen                                      (3/queued [2]; 0/screen [84, 84])\n",
      "  13/Attack_minimap                                     (3/queued [2]; 1/minimap [64, 64])\n",
      " 274/HoldPosition_quick                                 (3/queued [2])\n",
      " 220/Effect_Repair_screen                               (3/queued [2]; 0/screen [84, 84])\n",
      " 221/Effect_Repair_autocast                             ()\n",
      " 269/Harvest_Return_quick                               (3/queued [2])\n",
      "  79/Build_Refinery_screen                              (3/queued [2]; 0/screen [84, 84])\n",
      "  91/Build_SupplyDepot_screen                           (3/queued [2]; 0/screen [84, 84])\n",
      "  50/Build_EngineeringBay_screen                        (3/queued [2]; 0/screen [84, 84])\n",
      "  42/Build_Barracks_screen                              (3/queued [2]; 0/screen [84, 84])\n",
      "   6/select_idle_worker                                 (10/select_worker [4])\n",
      " 261/Halt_quick                                         (3/queued [2])\n",
      " 140/Cancel_quick                                       (3/queued [2])\n",
      " 335/Rally_Units_screen                                 (3/queued [2]; 0/screen [84, 84])\n",
      " 336/Rally_Units_minimap                                (3/queued [2]; 1/minimap [64, 64])\n",
      " 281/Lift_quick                                         (3/queued [2])\n",
      " 477/Train_Marine_quick                                 (3/queued [2])\n",
      "  43/Build_Bunker_screen                                (3/queued [2]; 0/screen [84, 84])\n",
      " 168/Cancel_Last_quick                                  (3/queued [2])\n",
      "  11/build_queue                                        (11/build_queue_id [10])\n",
      "   7/select_army                                        (7/select_add [2])\n",
      "   5/select_unit                                        (8/select_unit_act [4]; 9/select_unit_id [500])\n",
      "I0610 16:56:34.816698 4560528832 sc2_env.py:310] Episode finished. Outcome: [-1], reward: [-1], score: [20]\n",
      "I0610 16:56:38.850566 4560528832 sc2_env.py:240] Starting episode: 2\n",
      "  44/Build_CommandCenter_screen                         (3/queued [2]; 0/screen [84, 84])\n",
      "I0610 16:57:03.769397 4560528832 sc2_env.py:310] Episode finished. Outcome: [-1], reward: [-1], score: [10]\n",
      "I0610 16:57:07.700770 4560528832 sc2_env.py:240] Starting episode: 3\n",
      "I0610 16:57:32.390243 4560528832 sc2_env.py:310] Episode finished. Outcome: [-1], reward: [-1], score: [25]\n",
      "I0610 16:57:36.392425 4560528832 sc2_env.py:240] Starting episode: 4\n",
      "I0610 16:58:06.874982 4560528832 sc2_env.py:310] Episode finished. Outcome: [-1], reward: [-1], score: [25]\n",
      "I0610 16:58:10.849353 4560528832 sc2_env.py:240] Starting episode: 5\n",
      "I0610 16:58:37.917266 4560528832 sc2_env.py:310] Episode finished. Outcome: [-1], reward: [-1], score: [525]\n",
      "I0610 16:58:41.828097 4560528832 sc2_env.py:240] Starting episode: 6\n",
      "I0610 16:59:14.719199 4560528832 sc2_env.py:310] Episode finished. Outcome: [-1], reward: [-1], score: [620]\n",
      "I0610 16:59:18.595388 4560528832 sc2_env.py:240] Starting episode: 7\n",
      "I0610 16:59:46.926059 4560528832 sc2_env.py:310] Episode finished. Outcome: [-1], reward: [-1], score: [705]\n",
      "I0610 16:59:50.762461 4560528832 sc2_env.py:240] Starting episode: 8\n",
      "I0610 17:00:22.837107 4560528832 sc2_env.py:310] Episode finished. Outcome: [-1], reward: [-1], score: [3030]\n",
      "I0610 17:00:26.712828 4560528832 sc2_env.py:240] Starting episode: 9\n",
      "I0610 17:01:31.544255 4560528832 sc2_env.py:310] Episode finished. Outcome: [-1], reward: [-1], score: [2565]\n",
      "I0610 17:01:35.402614 4560528832 sc2_env.py:240] Starting episode: 10\n"
     ]
    }
   ],
   "source": [
    "!python -m pysc2.bin.agent \\\n",
    "--map Simple64 \\\n",
    "--agent smart_agent.SmartAgent \\\n",
    "--agent_race T \\\n",
    "--max_agent_steps 0 \\\n",
    "--norender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
